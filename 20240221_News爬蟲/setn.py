# -*- coding: utf-8 -*-
"""setn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BOTbxPEZ1LBwtzu_pLSTk8gmApVufQ4T
"""

from scrapy.spiders import CrawlSpider, Rule
from scrapy.linkextractors import LinkExtractor

# 三立新聞

#class 名稱Spider(CrawlSpider):
class SETNSpider(CrawlSpider):
    name = 'setn' # 定義spider 名稱 三立新聞
    custom_settings={
      'DOWNLOAD_DELAY':'3',  # 設定爬蟲時間延遲，數字可以再設定的大一點，但也不要讓自己等太久
      'FEED_EXPORT_ENCODING':'utf-8', # 設定文字編碼
      'SPIDER_LOADER_WARN_ONLY': True # 除錯用
    }
    allowed_domains = ['setn.com']  # 爬蟲執行網域，設定一個範圍，限定才不會抓到一堆不相干的事物

    start_urls = ['https://www.setn.com']  # 爬蟲起始網頁 | start_urls
    allow_list = ['https://www\.setn\.com/News\.aspx\?NewsID=.+'] # 需要分析之網址格式，用的是正規化(\d+/\d+) | allow_list

    # 當網址的格式符合allow_list的格式時，使用parse_item函式解析網頁，
    # 把網頁內的所有超連結加入追蹤清單中
    rules = [Rule(LinkExtractor(allow=allow_list), callback='parse_item', follow=True)]

    def parse_item(self, response):
        # 取出網頁新聞標題
        title = response.css('h1.news-title-3::text').get()
        # 利用開發者工具，看原網頁中，那標題的HTML、CSS寫法：h1 class = "news-title-3"

        # 取出網頁新聞內容
        ps = response.css('div#Content1 p::text').getall() # getall() 全部抓下來
        # div.Content1 p 之間的空白是為了抓到上下文都有；利用開發者工具，看原網頁中，那標題的HTML、CSS寫法
        content = ''.join(ps) # .join(ps) 將文本組合起來

        # 取出網址
        url = response.url
        yield {
          'title':title,
          'content':content,
          'url':url,
        }