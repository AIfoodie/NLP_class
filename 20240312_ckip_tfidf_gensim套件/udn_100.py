# -*- coding: utf-8 -*-
"""udn_100.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bkDt0xH1FB_JqnpcE77kupLgTT6y5Cjk
"""

import scrapy
from scrapy.spiders import CrawlSpider, Rule
from scrapy.linkextractors import LinkExtractor

class UdnSpider(CrawlSpider):
    name = 'udn'
    custom_settings = {
        'DOWNLOAD_DELAY': '3',
        'FEED_EXPORT_ENCODING': 'utf-8',
    }
    allowed_domains = ['udn.com']
    start_urls = ['https://udn.com/search/word/2/%E8%94%A1%E8%8B%B1%E6%96%87']
    allow_list = ['https://udn\.com/news/story/\d+/\d+']
    target_count = 100  # 預設要爬取的文章數量

    rules = [Rule(LinkExtractor(allow=allow_list), callback='parse_item', follow=True)]

    def __init__(self, *args, **kwargs):
        super(UdnSpider, self).__init__(*args, **kwargs)
        self.current_count = 0

    def parse_item(self, response):
        title = response.css('h1.article-content__title::text').get()
        ps = response.css('section.article-content__editor p::text').getall()
        content = ''.join(ps)
        url = response.url
        yield {
            'title': title,
            'content': content,
            'url': url,
        }

        self.current_count += 1
        if self.current_count >= self.target_count:
            self.crawler.engine.close_spider(self, '爬取完成')

    def closed(self, reason):
        self.log(f'Spider 已停止，總共爬取 {self.current_count} 篇文章。')